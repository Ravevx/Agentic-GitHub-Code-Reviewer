# ğŸ¤– AI Code Review Agent

> A fully local, privacy-safe multi-agent system that automatically reviews
> any GitHub repository and generates a detailed code review report
> powered by LangGraph + LM Studio. No API keys. No data leaves your machine.

![Python](https://img.shields.io/badge/Python-3.11+-blue)
![LangGraph](https://img.shields.io/badge/LangGraph-0.2+-green)
![LM Studio](https://img.shields.io/badge/LM%20Studio-Local%20LLM-purple)
![FastAPI](https://img.shields.io/badge/FastAPI-REST%20API-red)
![Streamlit](https://img.shields.io/badge/Streamlit-Web%20UI-orange)


## ğŸ“Œ What It Does

Give it any GitHub repository URL â†’ it automatically:

- ğŸ“¥ **Fetches** all source files via GitHub API
- ğŸ§  **Indexes** code into a ChromaDB vector store (RAG)
- ğŸ” **Reviews** each file for bugs, security issues, performance problems
- ğŸ’¡ **Suggests** minimal, targeted code fixes
- ğŸ“‹ **Generates** a full markdown report with executive summary + score


## ğŸ—ï¸ Architecture


GitHub URL
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LangGraph State Machine             â”‚
â”‚                                                  â”‚
â”‚  [FETCHER] â†’ [REVIEWER] â†’ [SUGGESTER] â†’ [SUMMARISER] â”‚
â”‚      â†“            â†“            â†“             â†“   â”‚
â”‚  GitHub API    RAG Query    Fix Suggest   .md Reportâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
review_YYYYMMDD_HHMMSS.md


### Agent Roles

| Agent | Responsibility |
|---|---|
| **Fetcher** | Calls GitHub API, extracts code from notebooks, builds ChromaDB RAG index |
| **Reviewer** | Strict static analysis â€” BUGS, SECURITY, PERFORMANCE, READABILITY |
| **Suggester** | Generates before/after code fixes for every finding |
| **Summariser** | Writes executive summary with VERDICT, SCORE, estimated fix time |



## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|---|---|
| Agent Orchestration | LangGraph |
| LLM Framework | LangChain + LangChain-OpenAI |
| Local LLM | LM Studio (Mistral / any model) |
| Embeddings | Nomic Embed Text (via LM Studio) |
| Vector Store | ChromaDB |
| Code Splitting | RecursiveCharacterTextSplitter (code-aware) |
| GitHub Integration | GitHub REST API v3 |
| Web UI | Streamlit |
| REST API | FastAPI + Uvicorn |



## âš™ï¸ Prerequisites

- Python 3.11+
- [LM Studio](https://lmstudio.ai/) installed and running
- A GitHub Personal Access Token
- A chat model loaded in LM Studio (e.g. `mistral-3b`)
- An embedding model loaded in LM Studio (e.g. `nomic-embed-text`)

---

## ğŸš€ Setup & Installation

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/ai-code-review-agent.git
cd ai-code-review-agent
```


### 2. Create and activate conda environment

```bash
conda create -n agent-local python=3.11
conda activate agent-local
```


### 3. Install dependencies

```bash
pip install -r requirements.txt
```


### 4. Create `.env` file

```bash
# .env
GITHUB_TOKEN=your_github_personal_access_token_here
```

> Get your token at: GitHub â†’ Settings â†’ Developer Settings â†’ Personal Access Tokens

### 5. Configure LM Studio

- Open LM Studio â†’ load a chat model (e.g. `Ministral 3B`)
- Load an embedding model (e.g. `nomic-embed-text`)
- Start the local server on `http://127.0.0.1:1234`
- Set **Context Length** to `8192` or higher in model settings


### 6. Update `config.py` with your model name

```python
LM_MODEL      = "your-model-name-in-lm-studio"
MAX_FILE_SIZE = 4000   # chars per file
MAX_FILES     = 5      # number of files to review
```


---

## â–¶ï¸ How to Run

### Option 1 â€” Command Line

```bash
python main.py
```

```
ğŸ”— Enter GitHub PR URL: https://github.com/owner/repo
...
ğŸ’¾ Report saved to: review_20260220_153700.md
```


---

### Option 2 â€” Streamlit Web UI

```bash
streamlit run app.py
```

Open `http://localhost:8501` in your browser.

```
1. Paste any GitHub repo URL
2. Click ğŸš€ Review PR
3. Wait ~2-5 minutes (depends on model + file count)
4. Download the .md report
```


---

### Option 3 â€” REST API

```bash
uvicorn api:app --host 0.0.0.0 --port 8000 --reload
```

**Swagger docs:** `http://localhost:8000/docs`

**Example request:**

```bash
curl -X POST http://localhost:8000/review \
  -H "Content-Type: application/json" \
  -d '{"pr_url": "https://github.com/owner/repo"}'
```

**Example response:**

```json
{
  "status": "success",
  "report": "# ğŸ¤– AI Code Review Report\n**Score:** 8.5/10 ..."
}
```


---

## ğŸ“Š Sample Output

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ¤– AI CODE REVIEW AGENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” [FETCHER] Fetching repository files from GitHub...
   ğŸ“Œ Repo: Machine-Learning | â­ 1 stars
   ğŸ“‚ Loading 9 code files...
âœ… [FETCHER] Loaded 9 files

ğŸ“š [RAG] Building code index...
   ğŸ“„ Indexed 63 chunks from 9 files
âœ… [RAG] Code index ready!

ğŸ” [REVIEWER] Analysing code...
   ğŸ” Reviewing: decision_tree.ipynb
   ğŸ” Reviewing: logistic_regression.ipynb
âœ… [REVIEWER] Reviewed 9 files

ğŸ’¡ [SUGGESTER] Generating fix suggestions...
âœ… [SUGGESTER] Generated suggestions for 9 files

ğŸ“‹ [SUMMARISER] Writing final report...
âœ… [SUMMARISER] Report complete!

ğŸ’¾ Report saved to: review_20260220_153700.md
```


### Report Preview

```
VERDICT: Approved with Minor Fixes
SCORE: 8.5/10
SECURITY RISK: Low

TOP 3 CRITICAL ISSUES:
- No input validation on file paths â†’ path traversal risk
- Silent failures on empty DataFrames â†’ AttributeError risk
- .tolist() on large datasets â†’ memory inefficiency

ESTIMATED FIX TIME: 12â€“16 hours
```


---

## ğŸ“ Project Structure

```
ai-code-review-agent/
â”‚
â”œâ”€â”€ main.py                  â† CLI runner
â”œâ”€â”€ api.py                   â† FastAPI REST server
â”œâ”€â”€ app.py                   â† Streamlit web UI
â”œâ”€â”€ llm.py                   â† LM Studio LLM factory
â”œâ”€â”€ config.py                â† Settings (URL, model, limits)
â”œâ”€â”€ test_lm.py               â† LM Studio connection test
â”œâ”€â”€ requirements.txt         â† Dependencies
â”œâ”€â”€ .env                     â† GitHub token (not committed)
â”‚
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ github_tools.py      â† GitHub API integration
â”‚
â”œâ”€â”€ rag/
â”‚   â””â”€â”€ code_store.py        â† ChromaDB vector index
â”‚
â”œâ”€â”€ graph/
â”‚   â””â”€â”€ review_graph.py      â† LangGraph pipeline
â”‚
â””â”€â”€ agents/
    â”œâ”€â”€ fetcher.py            â† Agent 1: Fetch + RAG
    â”œâ”€â”€ reviewer.py           â† Agent 2: Code review
    â”œâ”€â”€ suggester.py          â† Agent 3: Fix suggestions
    â””â”€â”€ summariser.py         â† Agent 4: Final report
```


---

## ğŸ”§ Configuration Reference

| Setting | Default | Description |
| :-- | :-- | :-- |
| `MAX_FILE_SIZE` | `4000` | Max chars per file sent to LLM |
| `MAX_FILES` | `5` | Max files to review per run |
| `LM_STUDIO_URL` | `http://127.0.0.1:1234/v1` | LM Studio server URL |
| `LM_MODEL` | `ministral-3b` | Model name in LM Studio |
| Context Length | `8192` | Set in LM Studio model settings |


---

## âš ï¸ Troubleshooting

| Error | Cause | Fix |
| :-- | :-- | :-- |
| `ConnectionRefusedError port 11434` | Ollama not running | Use LM Studio on port 1234 |
| `n_keep >= n_ctx` | Context window too small | Set LM Studio context to 8192+ |
| `ReadTimeout` | Files too large / model too slow | Reduce `MAX_FILE_SIZE` or `MAX_FILES` |
| `input must be a string` | Embedding format error | Add `check_embedding_ctx_length=False` |
| `cannot import text_splitter` | Old LangChain | `pip install langchain-text-splitters` |
| All files skipped | Extension not in list | Add extension to `CODE_EXTENSIONS` |


---

## ğŸ”® Future Improvements

- [ ] Async parallel file reviewing (9x speed improvement)
- [ ] Tree-sitter AST-based code chunking
- [ ] Persistent RAG cache (skip re-indexing same repo)
- [ ] Confidence scoring per finding
- [ ] LangGraph conditional branching (skip suggester for clean code)
- [ ] PR diff-only mode (review changed lines only)

---

## ğŸ“œ License

MIT License â€” free to use, modify, and distribute.

---

## ğŸ™ Acknowledgements

- [LangGraph](https://github.com/langchain-ai/langgraph) â€” multi-agent orchestration
- [LM Studio](https://lmstudio.ai/) â€” local LLM inference
- [ChromaDB](https://www.trychroma.com/) â€” vector storage
- [GitHub REST API](https://docs.github.com/en/rest) â€” repository access

```

***

Save this as `README.md` in the root of your project, push to GitHub, and it will render beautifully. ğŸš€```

#   A g e n t i c - G i t H u b - C o d e - R e v i e w e r 
 
 
